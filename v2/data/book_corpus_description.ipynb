{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/faridbala/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_tok(txt):\n",
    "\n",
    "    tokens = word_tokenize(txt)\n",
    "    \n",
    "# if wanna save points at the end of sentences,\n",
    "#          numbers and semi-words (two words with dash in between) uncomment line belove\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "withouth .isalpha: ['the','half-ling','book','kaylee','soderburg','copyright','2013','kaylee','.']\n",
    "['the','book','kaylee','soderburg','copyright','kaylee']\n",
    "\n",
    "with .isalpha: ['the','book','kaylee','soderburg','copyright','kaylee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bookcorpus/books_large_p1.txt\") as f:\n",
    "    doclist1 = [line.rstrip('\\n') for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = []\n",
    "lens1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doclist1:\n",
    "    words = sent_to_tok(sentence)\n",
    "    lens1.append(len(words))\n",
    "    words1.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in words1:\n",
    "    for word in i:\n",
    "        words1_.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_words1 = list(set(words1_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in books_large_p1 -----------------------: 40000000\n",
      "Number of words in books_large_p1 ---------------------------: 440569799\n",
      "Number of unique words in books_large_p1 --------------------: 494338\n",
      "average Number words in each sentence in books_large_p1 -----: 11.014244975\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences in books_large_p1 -----------------------:', len(doclist1))\n",
    "print('Number of words in books_large_p1 ---------------------------:', len(words1_))\n",
    "print('Number of unique words in books_large_p1 --------------------:', len(u_words1))\n",
    "print('average Number words in each sentence in books_large_p1 -----:', np.mean(lens1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('book1_unique.txt', 'w') as f:\n",
    "    for item in u_words1:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# book2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bookcorpus/books_large_p2.txt\") as f:\n",
    "    doclist2 = [line.rstrip('\\n') for line in f]\n",
    "#     docstr2 = ''.join(doclist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2=[]\n",
    "lens2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doclist2:\n",
    "    words = sent_to_tok(sentence)\n",
    "    lens2.append(len(words))\n",
    "    words2.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in words2:\n",
    "    for word in i:\n",
    "        words2_.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_words2 = list(set(words2_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in books_large_p2 -----------------------: 34004228\n",
      "Number of words in books_large_p2 ---------------------------: 364216419\n",
      "Number of unique words in books_large_p2 --------------------: 381383\n",
      "average Number words in each sentence in  books_large_p2 ----: 10.710915683779088\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences in books_large_p2 -----------------------:', len(doclist2))\n",
    "print('Number of words in books_large_p2 ---------------------------:', len(words2_))\n",
    "print('Number of unique words in books_large_p2 --------------------:', len(u_words2))\n",
    "print('average Number words in each sentence in books_large_p2 -----:', np.mean(lens2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('book2_unique.txt', 'w') as f:\n",
    "    for item in u_words2:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all togather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"book1_unique.txt\") as f:\n",
    "    doc1 = [line.rstrip('\\n') for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"book2_unique.txt\") as f:\n",
    "    doc2 = [line.rstrip('\\n') for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "875721"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ = doc1 + doc2\n",
    "len(all_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in books_corpus :  609758\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique words in books_corpus : ',len(set(all_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average Number words in each sentence in  books_corpus:  10.862580329389544\n"
     ]
    }
   ],
   "source": [
    "print('average Number words in each sentence in  books_corpus: ',(11.014244975 + 10.710915683779088) / 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
